{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "basics.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abishek/learning-anlp/blob/master/basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppqzoQgwvwLl"
      },
      "source": [
        "Shakespeare corpus has multiple plays in it. Each play is a document. I'll compute TF-IDF for all the terms across all the documents. Then I will try to use the compute Tf.Idf matrix as the basis for a cosine distance computation for a given query."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jszj7CaTQIRD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19790c18-45fa-425c-db7a-85d187f28444"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('shakespeare')\n",
        "from nltk.probability import FreqDist\n",
        "from nltk.corpus import stopwords, shakespeare\n",
        "import pandas as pd\n",
        "from math import log\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# shakespeare corpus has a few plays in it. let me pick the first one.\n",
        "print(shakespeare.fileids())\n",
        "\n",
        "def process_document(doc):\n",
        "  words = nltk.Text(nltk.corpus.shakespeare.words(doc))\n",
        "  #convert to small letters\n",
        "  words=[word.lower() for word in words if word.isalpha() ]\n",
        "  words=[word.lower() for word in words if word not in stop_words ]\n",
        "  return words\n",
        "\n",
        "all_words = []\n",
        "words_in_document = {}\n",
        "for play in shakespeare.fileids():\n",
        "  words = process_document(play)\n",
        "  all_words.extend(words)\n",
        "  words_in_document[play] = words\n",
        "  "
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]   Package shakespeare is already up-to-date!\n",
            "['a_and_c.xml', 'dream.xml', 'hamlet.xml', 'j_caesar.xml', 'macbeth.xml', 'merchant.xml', 'othello.xml', 'r_and_j.xml']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wiM1B0bysR47"
      },
      "source": [
        "The overall frequency distribution of words can be done using nltk FreqDist methods. Here is what it looks like."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_dWvEDKusY-2",
        "outputId": "ddec5415-05e8-42f0-bd40-785b2f59238f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "fDist = FreqDist(all_words)\n",
        "heading = ['Word','Frequency']\n",
        "tf_list = []\n",
        "for x,v in fDist.most_common(20):\n",
        "    tf_list.append((x,v))\n",
        "print(pd.DataFrame(tf_list,columns=heading))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      Word  Frequency\n",
            "0     thou       1141\n",
            "1    shall        816\n",
            "2      thy        670\n",
            "3     lord        631\n",
            "4     come        626\n",
            "5     thee        626\n",
            "6     good        606\n",
            "7   caesar        570\n",
            "8     love        567\n",
            "9    enter        563\n",
            "10     let        557\n",
            "11  antony        517\n",
            "12    well        502\n",
            "13   would        479\n",
            "14  hamlet        472\n",
            "15     man        436\n",
            "16      go        425\n",
            "17    know        410\n",
            "18    hath        408\n",
            "19    upon        403\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ODmulGpsepM"
      },
      "source": [
        "But let us compute Tf.Idf on a per-document basis. So the all_words distribution is less useful for this exercise. But all_words is the bag of words we'll use to compute the frequencies over.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKMqnm-vspLu",
        "outputId": "0a1bc758-921d-4a60-b46d-b2f9dfcad7d6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tfs = {d: {t: 0 for t in words_in_document[d]} for d in words_in_document.keys()}\n",
        "dfs = {t: 0 for t in all_words}\n",
        "tf_idfs = {d: {t: 0 for t in words_in_document[d]} for d in words_in_document.keys()}\n",
        "\n",
        "for term in all_words:\n",
        "  df = 0\n",
        "  for doc in words_in_document.keys():\n",
        "    tfs[doc][term] = words_in_document[doc].count(term) / len(words_in_document)\n",
        "    if term in words_in_document[doc]:\n",
        "      df += 1\n",
        "  dfs[term] = log( len(words_in_document.keys()) / df )\n",
        "\n",
        "for doc in words_in_document.keys():\n",
        "  for term in all_words:\n",
        "    tf_idfs[doc][term] = tfs[doc][term] * dfs[term]\n",
        "\n",
        "print(pd.DataFrame.from_dict(tf_idfs, orient='columns'))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "            a_and_c.xml  dream.xml  ...  othello.xml  r_and_j.xml\n",
            "tragedy        0.016691   0.016691  ...     0.016691     0.016691\n",
            "antony        33.530995   0.000000  ...     0.000000     0.086643\n",
            "cleopatra     47.480582   0.000000  ...     0.000000     0.173287\n",
            "dramatis       0.000000   0.000000  ...     0.000000     0.000000\n",
            "personae       0.000000   0.000000  ...     0.000000     0.000000\n",
            "...                 ...        ...  ...          ...          ...\n",
            "pothecary      0.000000   0.000000  ...     0.000000     0.259930\n",
            "jointure       0.000000   0.000000  ...     0.000000     0.259930\n",
            "sacrifices     0.000000   0.000000  ...     0.000000     0.259930\n",
            "glooming       0.000000   0.000000  ...     0.000000     0.259930\n",
            "punished       0.000000   0.000000  ...     0.000000     0.259930\n",
            "\n",
            "[11192 rows x 8 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}