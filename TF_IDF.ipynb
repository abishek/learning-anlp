{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TF-IDF.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNFx50RwJhud/D0cIE5GxiB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abishek/learning-anlp/blob/master/TF_IDF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9RHJI-OnyrEe",
        "colab_type": "text"
      },
      "source": [
        "The purpose of this note book is to understand basic tf-idf generation and maybe even understand cosine similarity and querying. I am constructing my own limited corpus of about 10 documents with simple sentences to implement this.\n",
        "\n",
        "Most of the code is self explanatory. I am also trying literate pattern. So you'll need to run all the code blocks in sequence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8A5MLrOq7kSb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "2d45698c-98c3-4d86-ded1-3e5109b0a65d"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.probability import  FreqDist\n",
        "import pandas as pd\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "my_corpus = {\n",
        "  'd0': 'Agricultural robots are used for harvesting of crops and weed control',\n",
        "  'd1': 'Weed control robots are more efficient alternatives to mass spraying of herbicide.',\n",
        "  'd2': 'Robots employing computer vision algorithms target weeds and spray herbicides with high precision.',\n",
        "  'd3': 'This reduces the development of herbicide resistance in weeds.',\n",
        "  'd4': 'Precision spraying done by robots also reduces the amount of herbicide that is sprayed on crops.',\n",
        "  'd5': 'Driverless tractors optimize operations on the farm and speed up the rate at which fields are tilled.',\n",
        "  'd6': 'The speed of the tractors, obstacle detection and avoidance, and the definition of preset routes is handled by an AI system.',\n",
        "  'd7': 'A supervisor monitors all operations from a central control room and takes remote control if necessary.',\n",
        "  'd8': 'Automated harvesting can compensate for labor shortages.',\n",
        "  'd9': 'Robotic crop pickers significantly reduce production costs for many crops.',\n",
        "}\n",
        "\n",
        "N_documents = 10\n",
        "words = []\n",
        "for document in my_corpus.values():\n",
        "  words.extend(document.split())\n",
        "\n",
        "# convert to lower case and remove stop words. this seems to take care of punctuations as well.\n",
        "words=[word.lower() for word in words if word.isalpha() ]\n",
        "words=[word.lower() for word in words if word not in stop_words]\n",
        "\n",
        "# I might need to lemmatize the content for better processing. \n",
        "# But I'll get to this later once the basics are thorough.\n",
        "print(\"There are {0} words in the corpus. {1} are unique.\".format(len(words), len(set(words))))\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "There are 74 words in the corpus. 61 are unique.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KhfCrbRI0h1",
        "colab_type": "text"
      },
      "source": [
        "We can calculate term frequencies using FreqDist method in nltk. I have computed using a simple list.count as well as using FreqDist. It seems they are the same."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4ERE2xaJAEL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "67f9d91b-62aa-455d-946f-23da63e477dc"
      },
      "source": [
        "heading = ['Word','Frequency']\n",
        "term_frequencies_custom = {}\n",
        "for term in set(words):\n",
        "  term_frequencies_custom[term] = words.count(term)\n",
        "# print(pd.DataFrame(term_frequencies_custom.items(), columns=heading))\n",
        "\n",
        "term_frequencies = FreqDist(words)\n",
        "tf_list = []\n",
        "for x,v in term_frequencies.most_common(5):\n",
        "    tf_list.append((x,v))\n",
        "print(pd.DataFrame(tf_list,columns=heading))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "         Word  Frequency\n",
            "0      robots          4\n",
            "1     control          4\n",
            "2  harvesting          2\n",
            "3        weed          2\n",
            "4    spraying          2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHbUy9pAMe_1",
        "colab_type": "text"
      },
      "source": [
        "Next we compute the document frequencies for these terms."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuWc9dOWMiau",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "fffecbe7-59f2-4b8a-c24f-9ce1659f4ab1"
      },
      "source": [
        "document_frequencies = {term: [] for term in term_frequencies.keys()}\n",
        "for term in term_frequencies.keys():\n",
        "  for k, document in my_corpus.items():\n",
        "    words_in_document = document.split()\n",
        "    words_in_document=[word.lower() for word in words_in_document if word.isalpha() ]\n",
        "    words=[word.lower() for word in words_in_document if word not in stop_words]\n",
        "    document_frequencies[term].append(words.count(term))\n",
        "\n",
        "freq_matrix = pd.DataFrame.from_dict(document_frequencies, orient='index', columns=my_corpus.keys())\n",
        "print(freq_matrix)\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "               d0  d1  d2  d3  d4  d5  d6  d7  d8  d9\n",
            "agricultural    1   0   0   0   0   0   0   0   0   0\n",
            "robots          1   1   1   0   1   0   0   0   0   0\n",
            "used            1   0   0   0   0   0   0   0   0   0\n",
            "harvesting      1   0   0   0   0   0   0   0   1   0\n",
            "crops           1   0   0   0   0   0   0   0   0   0\n",
            "...            ..  ..  ..  ..  ..  ..  ..  ..  ..  ..\n",
            "significantly   0   0   0   0   0   0   0   0   0   1\n",
            "reduce          0   0   0   0   0   0   0   0   0   1\n",
            "production      0   0   0   0   0   0   0   0   0   1\n",
            "costs           0   0   0   0   0   0   0   0   0   1\n",
            "many            0   0   0   0   0   0   0   0   0   1\n",
            "\n",
            "[61 rows x 10 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SUUfD_LPz2u",
        "colab_type": "text"
      },
      "source": [
        "Now that we have term and document frequencies, lets compute the Tf.IDf values for the terms in each document\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCa30I5-P40Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}