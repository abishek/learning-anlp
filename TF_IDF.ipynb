{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TF-IDF.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM6jZ9OON6d80Lr7S0xqiJY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abishek/learning-anlp/blob/master/TF_IDF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9RHJI-OnyrEe"
      },
      "source": [
        "The purpose of this note book is to understand basic tf-idf generation and maybe even understand cosine similarity and querying. I am constructing my own limited corpus of about 10 documents with simple sentences to implement this.\n",
        "\n",
        "Most of the code is self explanatory. I am also trying literate pattern. So you'll need to run all the code blocks in sequence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8A5MLrOq7kSb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20858263-e4aa-4fc5-b1ad-263f3486626d"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.probability import  FreqDist\n",
        "import pandas as pd\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "my_corpus = {\n",
        "  'd0': 'Agricultural robots are used for harvesting of crops and weed control',\n",
        "  'd1': 'Weed control robots are more efficient alternatives to mass spraying of herbicide.',\n",
        "  'd2': 'Robots employing computer vision algorithms target weeds and spray herbicides with high precision.',\n",
        "  'd3': 'This reduces the development of herbicide resistance in weeds.',\n",
        "  'd4': 'Precision spraying done by robots also reduces the amount of herbicide that is sprayed on crops.',\n",
        "  'd5': 'Driverless tractors optimize operations on the farm and speed up the rate at which fields are tilled.',\n",
        "  'd6': 'The speed of the tractors, obstacle detection and avoidance, and the definition of preset routes is handled by an AI system.',\n",
        "  'd7': 'A supervisor monitors all operations from a central control room and takes remote control if necessary.',\n",
        "  'd8': 'Automated harvesting can compensate for labor shortages.',\n",
        "  'd9': 'Robotic crop pickers significantly reduce production costs for many crops.',\n",
        "}\n",
        "\n",
        "N_documents = 10\n",
        "words = []\n",
        "for document in my_corpus.values():\n",
        "  words.extend(document.split())\n",
        "\n",
        "# convert to lower case and remove stop words. this seems to take care of punctuations as well.\n",
        "words=[word.lower() for word in words if word.isalpha() ]\n",
        "words=[word.lower() for word in words if word not in stop_words]\n",
        "\n",
        "# I might need to lemmatize the content for better processing. \n",
        "# But I'll get to this later once the basics are thorough.\n",
        "print(\"There are {0} words in the corpus. {1} are unique.\".format(len(words), len(set(words))))\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "There are 74 words in the corpus. 61 are unique.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KhfCrbRI0h1"
      },
      "source": [
        "We can calculate term frequencies using FreqDist method in nltk. I have computed using a simple list.count. It seems this is the same as what FreqDist will give me. But the catch is, these for the entire corpus as a single document. I am interested in per document numbers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4ERE2xaJAEL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2ea16f1-5b23-451a-8c9e-24d3028763d6"
      },
      "source": [
        "heading = ['Word','Tf']\n",
        "term_frequencies_custom = {}\n",
        "for term in set(words):\n",
        "  term_frequencies_custom[term] = words.count(term)\n",
        "custom_tf_df = pd.DataFrame(term_frequencies_custom.items(), columns=heading)\n",
        "custom_tf_df.sort_values(by=['Tf', 'Word'], inplace=True, ascending=[False, True])\n",
        "print(custom_tf_df[:5])\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "          Word  Frequency\n",
            "52     control          4\n",
            "32      robots          4\n",
            "6   harvesting          2\n",
            "31   herbicide          2\n",
            "1   operations          2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHbUy9pAMe_1"
      },
      "source": [
        "Let's compute TF.IDF for each of these terms. TF is the same as above but taken per-document.\n",
        "\n",
        "We define IDF as follows:\n",
        "\n",
        "$IDF_t = log(\\frac{N}{D_{ft}})$\n",
        "\n",
        "Where N is the number of documents, $D_{ft}$ is the number of documents containing the term t."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuWc9dOWMiau",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e9061f7-0961-4d38-93f2-d3b8dae8823b"
      },
      "source": [
        "from math import log\n",
        "\n",
        "tfs = {term: {} for term in term_frequencies_custom.keys()}\n",
        "idfs = {term: 0 for term in term_frequencies_custom.keys()}\n",
        "for term in term_frequencies_custom.keys():\n",
        "  tfs[term] = {k: 0 for k in my_corpus.keys()}\n",
        "  dft = 0\n",
        "  for k, document in my_corpus.items():\n",
        "    words_in_document = document.split()\n",
        "    words_in_document=[word.lower() for word in words_in_document if word.isalpha() ]\n",
        "    words=[word.lower() for word in words_in_document if word not in stop_words]\n",
        "    if term in words:\n",
        "      dft += 1\n",
        "      tfs[term][k] = words.count(term)\n",
        "  idfs[term] = log(N_documents/dft)\n",
        "\n",
        "tf_idfs = {term: {k: 0 for k in my_corpus.keys()} for term in term_frequencies_custom.keys()}\n",
        "for term in term_frequencies_custom.keys():\n",
        "  for k in my_corpus.keys():\n",
        "    tf_idfs[term][k] = tfs[term][k] * idfs[term]\n",
        "\n",
        "print(pd.DataFrame.from_dict(tf_idfs, orient='index'))\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                    d0        d1        d2  ...        d7        d8        d9\n",
            "reduce        0.000000  0.000000  0.000000  ...  0.000000  0.000000  2.302585\n",
            "operations    0.000000  0.000000  0.000000  ...  1.609438  0.000000  0.000000\n",
            "compensate    0.000000  0.000000  0.000000  ...  0.000000  2.302585  0.000000\n",
            "crops         2.302585  0.000000  0.000000  ...  0.000000  0.000000  0.000000\n",
            "alternatives  0.000000  2.302585  0.000000  ...  0.000000  0.000000  0.000000\n",
            "...                ...       ...       ...  ...       ...       ...       ...\n",
            "farm          0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000\n",
            "remote        0.000000  0.000000  0.000000  ...  2.302585  0.000000  0.000000\n",
            "spray         0.000000  0.000000  2.302585  ...  0.000000  0.000000  0.000000\n",
            "efficient     0.000000  2.302585  0.000000  ...  0.000000  0.000000  0.000000\n",
            "done          0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000\n",
            "\n",
            "[61 rows x 10 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}